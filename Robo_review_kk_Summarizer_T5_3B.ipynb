{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install transformers huggingface_hub\n",
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-3b automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the T5-3B model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-3b')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-3b')\n",
    "\n",
    "# Load the T5-11B model and tokenizer\n",
    "#model = T5ForConditionalGeneration.from_pretrained('t5-11b')\n",
    "#tokenizer = T5Tokenizer.from_pretrained('t5-11b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        category_name                                       reviews.text\n",
      "0           Fire HD 8  this product so far has not disappointed. my c...\n",
      "1           Fire KIDS  the tablet is very light and streams well. i o...\n",
      "2       Fire Tablet 7  good basic tablet for checking email , web bro...\n",
      "3              Kindle  very lightweight and portable with excellent b...\n",
      "4  Speakers/Streaming  i really enjoy the echo. i got an echo dot and...\n"
     ]
    }
   ],
   "source": [
    "# LOAD the DATA (the output DS from K-means clustering)\n",
    "file_path = 'categorized_dataset_k5_with_names.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Data cleaning: Convert reviews.text to lowercase and remove NULLs\n",
    "df['reviews.text'] = df['reviews.text'].astype(str).str.lower()\n",
    "df = df[df['reviews.text'].notnull()]\n",
    "\n",
    "# Group all reviews under each Category \n",
    "grouped_reviews = df.groupby('category_name')['reviews.text'].apply(lambda texts: ' '.join(texts)).reset_index() \n",
    "\n",
    "print(grouped_reviews.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the CUDA cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        category_name                                       reviews.text  \\\n",
      "0           Fire HD 8  this product so far has not disappointed. my c...   \n",
      "1           Fire KIDS  the tablet is very light and streams well. i o...   \n",
      "2       Fire Tablet 7  good basic tablet for checking email , web bro...   \n",
      "3              Kindle  very lightweight and portable with excellent b...   \n",
      "4  Speakers/Streaming  i really enjoy the echo. i got an echo dot and...   \n",
      "\n",
      "                                        blog_summary  \\\n",
      "0  .: Fire HD 8.: Fire HD 8. Write a blog-style s...   \n",
      "1  KIDS.               i   . i love it. i love it...   \n",
      "2  this is a great tablet for the price. i got it...   \n",
      "3  the kindle oasis is the smallest and lightest ...   \n",
      "4  alexa is a great assistant. i use it all the t...   \n",
      "\n",
      "                                          highlights  \\\n",
      "0  the kindle fire hd 8 is a great tablet. it's s...   \n",
      "1                                                      \n",
      "2                                           focus on   \n",
      "3                                                      \n",
      "4  focus on product strengths and customer satisf...   \n",
      "\n",
      "                                              issues  \n",
      "0  write a blog-style summary covering the specif...  \n",
      "1                                                     \n",
      "2                                                     \n",
      "3  focus on customer complaints, problems, or dis...  \n",
      "4  focus on product strengths and customer satisf...  \n"
     ]
    }
   ],
   "source": [
    "# VERSION 1\n",
    "\n",
    "# Function to remove overlapping content\n",
    "def remove_overlapping_content(summary, highlights, issues):\n",
    "    # Convert to lowercase and remove excess spaces\n",
    "    summary = summary.lower().strip()\n",
    "    highlights = [highlight.lower().strip() for highlight in highlights.split('. ') if highlight not in summary]\n",
    "    issues = [issue.lower().strip() for issue in issues.split('. ') if issue not in summary and issue not in highlights]\n",
    "    \n",
    "    return '. '.join(highlights), '. '.join(issues)\n",
    "\n",
    "# Function to generate summary, highlights, and issues with distinct instructions\n",
    "def generate_summary_highlights_issues(text, category_name):\n",
    "    # Generate blog-style summary\n",
    "    #summary_prompt = f\"Write a blog-style summary covering the specifications, features, and configurations of a product in the category: {category_name}. Here are the reviews: \" + text\n",
    "    summary_prompt = f\"Write a blog-style summary covering the specifications, features, and performance of a product in the category: {category_name}. Reviews: \" + text\n",
    "    summary_ids = model.generate(tokenizer.encode(summary_prompt, return_tensors=\"pt\", max_length=1024, truncation=True), max_length=500, num_beams=4, length_penalty=2.0, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Generate highlights focusing on positive aspects not in the summary\n",
    "    #highlight_prompt = f\"List 3-4 positive features and advantages over competitors that are not mentioned in the summary: {summary}. Focus on product strengths and customer satisfaction for the product in the category: {category_name}. Here are the reviews: \" + text\n",
    "    highlight_prompt = f\"List 3-4 positive features and advantages of the product in the category: {category_name}. Reviews: \" + text\n",
    "    highlights_ids = model.generate(tokenizer.encode(highlight_prompt, return_tensors=\"pt\", max_length=1024, truncation=True), max_length=150, num_beams=4, length_penalty=1.5, early_stopping=True)\n",
    "    highlights = tokenizer.decode(highlights_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Generate issues focusing on negative aspects not in the summary or highlights\n",
    "    #issue_prompt = f\"List 2-3 issues or disadvantages with the product that are not mentioned in the summary: {summary} or highlights: {highlights}. Focus on customer complaints, problems, or disadvantages compared to competitors for the product in the category: {category_name}. Here are the reviews: \" + text\n",
    "    issue_prompt = f\"List 2-3 issues or disadvantages of the product in the category: {category_name}. Reviews: \" + text\n",
    "    issues_ids = model.generate(tokenizer.encode(issue_prompt, return_tensors=\"pt\", max_length=1024, truncation=True), max_length=100, num_beams=4, length_penalty=1.5, early_stopping=True)\n",
    "    issues = tokenizer.decode(issues_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Remove overlaps between sections\n",
    "    highlights, issues = remove_overlapping_content(summary, highlights, issues)\n",
    "\n",
    "    return summary, highlights, issues\n",
    "\n",
    "# Apply the function to each category\n",
    "grouped_reviews['blog_summary'], grouped_reviews['highlights'], grouped_reviews['issues'] = zip(*grouped_reviews.apply(\n",
    "    lambda row: generate_summary_highlights_issues(row['reviews.text'], row['category_name']), axis=1))\n",
    "\n",
    "# Inspect the generated summaries\n",
    "print(grouped_reviews.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save blog-style and compact summaries to an HTML file\n",
    "with open('T5_3B_summaries_v2.html', 'w') as f:\n",
    "    f.write('<html><body>')\n",
    "    \n",
    "    for index, row in grouped_reviews.iterrows():\n",
    "        f.write(f\"<h2>Product: {row['category_name']}</h2>\")\n",
    "        \n",
    "        f.write(\"<h3>Summary</h3>\")\n",
    "        f.write(f\"<p>{row['blog_summary']}</p>\")\n",
    "        \n",
    "        f.write(\"<h3>Highlights</h3>\")\n",
    "        f.write(f\"<ul><li>{'</li><li>'.join(row['highlights'].split('. '))}</li></ul>\")\n",
    "        \n",
    "        f.write(\"<h3>Issues</h3>\")\n",
    "        f.write(f\"<ul><li>{'</li><li>'.join(row['issues'].split('. '))}</li></ul>\")\n",
    "        \n",
    "        f.write('<hr>')\n",
    "    \n",
    "    f.write('</body></html>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./summarizer-T5_3B/tokenizer_config.json',\n",
       " './summarizer-T5_3B/special_tokens_map.json',\n",
       " './summarizer-T5_3B/spiece.model',\n",
       " './summarizer-T5_3B/added_tokens.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model and tokenizer\n",
    "model.save_pretrained(\"./summarizer-T5_3B\")\n",
    "tokenizer.save_pretrained(\"./summarizer-T5_3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Function to load and return the HTML content\n",
    "def display_html():\n",
    "    with open(\"your_output.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "        html_content = file.read()\n",
    "    return html_content\n",
    "\n",
    "# Create Gradio interface with HTML component\n",
    "demo = gr.Interface(fn=display_html, inputs=[], outputs=gr.HTML())\n",
    "\n",
    "# Launch the app\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Function to dynamically generate HTML based on input\n",
    "def generate_html(category):\n",
    "    # Replace this with the actual HTML generation logic\n",
    "    html_output = f\"<h1>Product Category: {category}</h1><p>Generated blog content goes here...</p>\"\n",
    "    return html_output\n",
    "\n",
    "# Create Gradio interface with a dropdown and HTML output\n",
    "demo = gr.Interface(fn=generate_html, inputs=gr.Dropdown(choices=[\"Category 1\", \"Category 2\", \"Category 3\"]), outputs=gr.HTML())\n",
    "\n",
    "# Launch the app\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
