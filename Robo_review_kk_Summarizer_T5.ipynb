{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install transformers huggingface_hub\n",
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the T5 model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        category_name                                       reviews.text\n",
      "0           Fire HD 8  this product so far has not disappointed. my c...\n",
      "1           Fire KIDS  the tablet is very light and streams well. i o...\n",
      "2       Fire Tablet 7  good basic tablet for checking email , web bro...\n",
      "3              Kindle  very lightweight and portable with excellent b...\n",
      "4  Speakers/Streaming  i really enjoy the echo. i got an echo dot and...\n"
     ]
    }
   ],
   "source": [
    "# LOAD the DATA (the output DS from K-means clustering)\n",
    "file_path = 'categorized_dataset_k5_with_names.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Data cleaning: Convert reviews.text to lowercase and remove NULLs\n",
    "df['reviews.text'] = df['reviews.text'].astype(str).str.lower()\n",
    "df = df[df['reviews.text'].notnull()]\n",
    "\n",
    "# Group all reviews under each Category \n",
    "grouped_reviews = df.groupby('category_name')['reviews.text'].apply(lambda texts: ' '.join(texts)).reset_index() \n",
    "\n",
    "\"\"\"\n",
    "# We group by both category_name and label to ensure we retain the sentiment\n",
    "grouped_reviews = df.groupby(['category_name', 'label'])['reviews.text'].apply(lambda texts: ' '.join(texts)).reset_index() \n",
    "\"\"\"\n",
    "\n",
    "print(grouped_reviews.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# VERSION 2\n",
    "# STEP to generate SUMMARY - one Blog style large summary and one compact summary for Highlights/Issues - in HTML file for deployment in Gradio\n",
    "\n",
    "\n",
    "# List of common pronouns to remove\n",
    "pronouns = ['i', 'you', 'he', 'she', 'we', 'they', 'my', 'your', 'his', 'her', 'our', 'their', 'us', 'me', 'll', 'have']\n",
    "\n",
    "# Function to remove pronouns\n",
    "def remove_pronouns(text):\n",
    "    text = re.sub(r'\\b(?:{})\\b'.format('|'.join(pronouns)), '', text)\n",
    "    return text\n",
    "\n",
    "# Summarization function for blog-style summaries\n",
    "def generate_blog_style_summary(text):\n",
    "    cleaned_text = remove_pronouns(text)\n",
    "    input_text = \"summarize: Focus on the product features and exclude any personal mentions. \" + cleaned_text\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    summary_ids = model.generate(inputs, max_length=300, min_length=150, num_beams=6, length_penalty=2.5, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Function for compact summary on highlights and issues\n",
    "def generate_compact_summary(text):\n",
    "    cleaned_text = remove_pronouns(text)\n",
    "    input_text = \"summarize: Highlight 2-4 key features and mention 2-4 issues of this product. \" + cleaned_text\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    summary_ids = model.generate(inputs, max_length=100, min_length=50, num_beams=4, length_penalty=1.5, early_stopping=True)\n",
    "    compact_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return compact_summary\n",
    "\n",
    "# Generate both blog-style and compact summaries\n",
    "grouped_reviews['blog_style_summary'] = grouped_reviews['reviews.text'].apply(generate_blog_style_summary)\n",
    "grouped_reviews['compact_summary'] = grouped_reviews['reviews.text'].apply(generate_compact_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save blog-style and compact summaries to an HTML file\n",
    "with open('product summaries_T5_v2.html', 'w') as f:\n",
    "    f.write('<html><body>')\n",
    "    \n",
    "    for index, row in grouped_reviews.iterrows():\n",
    "        f.write(f\"<h2>Product name: {row['category_name']}</h2>\")\n",
    "        \n",
    "        f.write(\"<h3>Summary</h3>\")\n",
    "        f.write(f\"<p>{row['blog_style_summary']}</p>\")\n",
    "        \n",
    "        f.write(\"<h3>Highlights & Issues</h3>\")\n",
    "        f.write(f\"<p>{row['compact_summary']}</p>\")\n",
    "        f.write('<hr>')\n",
    "    \n",
    "    f.write('</body></html>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer\n",
    "model.save_pretrained(\"./summarizer-T5_large\")\n",
    "tokenizer.save_pretrained(\"./summarizer-T5_large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSION 4 - modified version 3 (to go back to blog level summary & separate highlighs&issues)\n",
    "\"\"\" \n",
    "\n",
    "# List of common pronouns to remove (including 'us', 'me', etc.)\n",
    "pronouns = ['i', 'you', 'he', 'she', 'we', 'they', 'my', 'your', 'his', 'her', 'our', 'their', 'us', 'me', 'll', 'have']\n",
    "\n",
    "# Function to remove pronouns\n",
    "def remove_pronouns(text):\n",
    "    text = re.sub(r'\\b(?:{})\\b'.format('|'.join(pronouns)), '', text)\n",
    "    return text\n",
    "\n",
    "# Generate blog-style summary (previous version you liked)\n",
    "def generate_blog_style_summary(text):\n",
    "    cleaned_text = remove_pronouns(text)\n",
    "    input_text = \"summarize: Focus on the product's features and exclude any personal mentions. \" + cleaned_text\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    summary_ids = model.generate(inputs, max_length=300, min_length=150, num_beams=6, length_penalty=2.5, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate bullet points for highlights and issues based on labels\n",
    "def generate_bullet_points(df, label):\n",
    "    text = ' '.join(df[df['label'] == label]['reviews.text'].tolist())\n",
    "    cleaned_text = remove_pronouns(text)\n",
    "    \n",
    "    if label == 2:  # Highlights\n",
    "        input_text = \"summarize: Provide 2-4 bullet points with product highlights. \" + cleaned_text\n",
    "    elif label == 0:  # Issues\n",
    "        input_text = \"summarize: Provide 2-4 bullet points with product issues. \" + cleaned_text\n",
    "    \n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    summary_ids = model.generate(inputs, max_length=150, min_length=50, num_beams=8, length_penalty=1.5, early_stopping=True)\n",
    "    bullet_points = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return bullet_points\n",
    "\n",
    "# Apply the summaries and bullet points generation\n",
    "grouped_reviews['blog_style_summary'] = grouped_reviews['reviews.text'].apply(generate_blog_style_summary)\n",
    "grouped_reviews['highlights'] = grouped_reviews.apply(lambda row: generate_bullet_points(df[df['category_name'] == row['category_name']], label=2), axis=1)\n",
    "grouped_reviews['issues'] = grouped_reviews.apply(lambda row: generate_bullet_points(df[df['category_name'] == row['category_name']], label=0), axis=1)\n",
    "\n",
    "# Save to HTML with bullet points for highlights and issues\n",
    "with open('product_summaries_with_highlights_issues_v4.html', 'w') as f:\n",
    "    f.write('<html><body>')\n",
    "    \n",
    "    for index, row in grouped_reviews.iterrows():\n",
    "        f.write(f\"<h2>Prodct: {row['category_name']}</h2>\")\n",
    "        \n",
    "        f.write(\"<h3>Summary</h3>\")\n",
    "        f.write(f\"<p>{row['blog_style_summary']}</p>\")\n",
    "        \n",
    "        f.write(\"<h3>Highlights</h3>\")\n",
    "        f.write(f\"<ul><li>{'</li><li>'.join(row['highlights'].split('. '))}</li></ul>\")\n",
    "        \n",
    "        f.write(\"<h3>Issues</h3>\")\n",
    "        f.write(f\"<ul><li>{'</li><li>'.join(row['issues'].split('. '))}</li></ul>\")\n",
    "        \n",
    "        f.write('<hr>')\n",
    "    \n",
    "    f.write('</body></html>') \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
